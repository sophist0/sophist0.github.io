<html>
<head>
<title>Data Exploration and XGBoost - Notes</title>
<link rel="stylesheet" type="text/css" href="styles.css">
<link rel="stylesheet" type="text/css" href="xgboost.css">
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div class="hspacer"></div>
<div class="content_block">
<h1>Introduction</h1>
<hr>
<p>
XGBoost is one of the ML models that I've run across a lot but never played around with. So this is my attempt at getting familiar with the python implementation of XGBoost. Basically this involved following the docs here: <a href="https://xgboost.readthedocs.io/en/latest/"> https://xgboost.readthedocs.io/en/latest/</a>
</p>

<h1>Data Exploration</h1>
<hr>
<p>
I'm interested in cooking. So I decided to look for a dataset of recipes and found <a href="https://www.kaggle.com/kaggle/recipe-ingredients-dataset">https://www.kaggle.com/kaggle/recipe-ingredients-dataset</a>. 
</p>
<p>
This dataset consists of recipes classified by cuisine with the recipe ingredients as features. Specifically the dataset consists of 39774 recipes, 20 cuisines, and 6714 unique ingredients. Arguably this dataset could use cleaning in the sense that some features could be consolidated for instance "olive oil" and "extra-virgin olive oil", on the other hand different cuisines may use different words for identical or substitutable ingredients. I chose not to consolidate any features. 
</p>
<p>
The cuisines and their frequency in the dataset are given in the figure below.
<img class="img_cnr" src="images/cuisine_num_recipes_edit.png">
</p>
<p>
The twenty most frequent ingredients in all cuisine recipes in the dataset are given in the figure below.
<img class="img_cnr" src="images/ingredient_num_recipes_edit.png">
</p>
<p>
Eight most frequent ingredients of each cuisine which are not in the set of the eight most frequent ingredients of any other cuisine in the dataset are given below.
<br>
<br>
<table class="center">
  <tr>
    <th><b>Cuisine</b></th>
    <th><b>Ingredients</b></th>
  </tr>
  <tr>
    <td>brazilian</td>
    <td>cachaca, lime</td>
  </tr>
  <tr>
    <td>british</td>
    <td>milk</td>
  </tr>
  <tr>
    <td>cajun_creole</td>
    <td>cajun seasoning, cayenne pepper, green bell pepper</td>
  </tr>
  <tr>
    <td>chinese</td>
    <td>corn starch</td>
  </tr>
  <tr>
    <td>filipino</td>
    <td>oil</td>
  </tr>
  <tr>
    <td>french</td>
    <td></td>
  </tr>
  <tr>
    <td>greek</td>
    <td>fresh lemon juice, feta cheese crumbles, dried oregano</td>
  </tr>
  <tr>
    <td>indian</td>
    <td>cumin seed, ground turmeric, garam masala</td>
  </tr>
  <tr>
    <td>irish</td>
    <td>baking soda, potatoes</td>
  </tr>
  <tr>
    <td>italian</td>
    <td>grated parmesan cheese</td>
  </tr>
  <tr>
    <td>jamaican</td>
    <td>dried thyme, scallions, ground allspice</td>
  </tr>
  <tr>
    <td>japanese</td>
    <td>rice vinegar, sake, mirin</td>
  </tr>
  <tr>
    <td>korean</td>
    <td>seseme seeds</td>
  </tr>
  <tr>
    <td>mexican</td>
    <td>jalapeno chilies, chili powder</td>
  </tr>
  <tr>
    <td>moroccan</td>
    <td>ground ginger, ground cinnamon</td>
  </tr>
  <tr>
    <td>russian</td>
    <td></td>
  </tr>
  <tr>
    <td>southern_us</td>
    <td></td>
  </tr>
  <tr>
    <td>spanish</td>
    <td>tomatoes</td>
  </tr>
  <tr>
    <td>thai</td>
    <td>coconut milk</td>
  </tr>
  <tr>
    <td>vietnamese</td>
    <td>shallots, carrots</td>
  </tr>
</table> 
<br>
If one fills in this table for the thirty most frequent ingredients rather than the eight, then every cuisine has ingredients in the column of ingredients. As we will see the eight most frequent elements produce nicer graphical representation. But the point I'm trying to drive home is that if we only used the thirty most frequent ingredients of each cuisine we should be able to categorize a lot of recipes. In other words classifying recipes according to cuisine using their ingredients appears to be a solvable problem. 
</p>
<p>
Below is a bipartite graph representation of subsets of the eight most frequent ingredients for each cuisine. The white nodes are the cuisines, the edges to blue nodes are subsets of that cuisines eight most frequent ingredients. If there is more than one edge to a blue node, that nodes ingredients are in the set of the eight most frequent ingredients for each connected cuisine. For instance both Chinese and Korean recipes frequently use green onions and sesame oil.
<img class="img_cnr" src="images/cuisine_ingredients_n8.png">
<br>
Notice that the ingredients in the center of the graph such as garlic, onions, salt, sugar, and water are connected to most cuisines. Therefore their presence in a recipe likely tells us little about the recipes cuisine. Or more formally if all ingredient nodes represent sufficiently frequent ingredients, then highest degree nodes in the bipartite representation above likely have little power to classify recipes by cuisine. I hope to test this hypothesis below.
</p>

<h1>XGBoost Paramater Selection</h1>
<hr>
<p>
XGBoost has a lot of parameters, see <a href="https://xgboost.readthedocs.io/en/latest/parameter.html">https://xgboost.readthedocs.io/en/latest/parameter.html</a>. I chose to focus on sweeping over the following parameters which produces 216 potential models.
<br><br>
<table class="center">
  <tr>
    <th><b>Parameter</b></th>
    <th><b>Description</b></th>
    <th><b>Range</b></th>
    <th><b>Default</b></th>
    <th><b>Sweep 1 Values</b></th>
  </tr>
  <tr>
    <td>eta</td>
    <td>learning rate</td>
    <td>[0,1]</td>
    <td>0.3</td>
    <td>0.1,0.5,1</td>
  </tr>
  <tr>
    <td>gamma</td>
    <td>min leaf loss to split a leaf</td>
    <td>[0,inf]</td>
    <td>0</td>
    <td>0,0.3</td>
  </tr>
  <tr>
    <td>max_depth</td>
    <td>maximum tree depth</td>
    <td>[0,inf]</td>
    <td>6</td>
    <td>2,4,6</td>
  </tr>
  <tr>
    <td>lambda</td>
    <td>L2 regulation</td>
    <td></td>
    <td>1</td>
    <td>0,1,3</td>
  </tr>
  <tr>
    <td>subsample</td>
    <td>fraction of data sampled before growing each tree</td>
    <td>(0,1]</td>
    <td>1</td>
    <td>0.7,1</td>
  </tr>
  <tr>
    <td>num_round</td>
    <td>number of boosting rounds usually 2</td>
    <td></td>
    <td></td>
    <td>2,4</td>
  </tr>
</table>
<br>
Setting aside 10% of the dataset as a testing set and using 5-fold cross validation, the top ten average accuracies and the corresponding parameter sets are given in the table below.
<br><br>
<table class="center">
  <tr>
    <td>Model</td>
    <td>S1M1</td>
    <td>S1M2</td>
    <td>S1M3</td>
    <td>S1M4</td>
    <td>S1M5</td>
    <td>S1M6</td>
    <td>S1M7</td>
    <td>S1M8</td>
    <td>S1M9</td>
    <td>S1M10</td>
  </tr>
  <tr>
    <td>Accuracy</td>
    <td>0.653</td>
    <td>0.653</td>
    <td>0.648</td>
    <td>0.648</td>
    <td>0.647</td>
    <td>0.646</td>
    <td>0.641</td>
    <td>0.641</td>
    <td>0.635</td>
    <td>0.634</td>
  </tr>
  <tr>
    <td>eta</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
  </tr>
  <tr>
    <td>gamma</td>
    <td>0</td>
    <td>0.3</td>
    <td>0</td>
    <td>0.3</td>
    <td>0</td>
    <td>0.3</td>
    <td>0</td>
    <td>0.3</td>
    <td>0</td>
    <td>0.3</td>
  </tr>
  <tr>
    <td>max_depth</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
  </tr>
  <tr>
    <td>lambda</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>1</td>
    <td>1</td>
    <td>1</td>
    <td>3</td>
    <td>3</td>
  </tr>
  <tr>
    <td>subsample</td>
    <td>1</td>
    <td>1</td>
    <td>0.7</td>
    <td>0.7</td>
    <td>1</td>
    <td>1</td>
    <td>0.7</td>
    <td>0.7</td>
    <td>1</td>
    <td>1</td>
  </tr>
  <tr>
    <td>num_round</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
  </tr>
</table>
</p>
<p>
Given this parameter sweep it appears we can fix eta=0.5 and it doesn't appear gamma has a large effect so lets fix gamma=0. Its also tempting to set lambda=0 and subsample=1 but I'm not going to fix those values as both parameters are attempts at avoiding over fitting. Finally the fact that max_depth and num_round are at the maximum of their swept ranges suggest that they should both be increased and that the model is under fitting the validation data. So lets try another round of parameter fitting sweeping over the following values.
<br><br>
<table class="center">
  <tr>
    <th><b>Parameter</b></th>
    <th><b>Sweep 2 Values</b></th>
  </tr>
  <tr>
    <td>eta</td>
    <td>0.5</td>
  </tr>
  <tr>
    <td>gamma</td>
    <td>0</td>
  </tr>
  <tr>
    <td>max_depth</td>
    <td>8,10,12,14</td>
  </tr>
  <tr>
    <td>lambda</td>
    <td>0,1</td>
  </tr>
  <tr>
    <td>subsample</td>
    <td>0.7,1</td>
  </tr>
  <tr>
    <td>num_round</td>
    <td>6,8,10,12</td>
  </tr>
</table>
</p>
<p>
This gives 64 possible models. Again the top ten results are,
<br><br>
<table class="center">
  <tr>
    <td>Model</td>
    <td>S2M1</td>
    <td>S2M2</td>
    <td>S2M3</td>
    <td>S2M4</td>
    <td>S2M5</td>
    <td>S2M6</td>
    <td>S2M7</td>
    <td>S2M8</td>
    <td>S2M9</td>
    <td>S2M10</td>
  </tr>
  <tr>
    <td>Accuracy</td>
    <td>0.720</td>
    <td>0.718</td>
    <td>0.717</td>
    <td>0.717</td>
    <td>0.714</td>
    <td>0.713</td>
    <td>0.713</td>
    <td>0.713</td>
    <td>0.713</td>
    <td>0.713</td>
  </tr>
  <tr>
    <td>eta</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
  </tr>
  <tr>
    <td>gamma</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>max_depth</td>
    <td>14</td>
    <td>12</td>
    <td>14</td>
    <td>14</td>
    <td>14</td>
    <td>12</td>
    <td>10</td>
    <td>12</td>
    <td>12</td>
    <td>14</td>
  </tr>
  <tr>
    <td>lambda</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>subsample</td>
    <td>1</td>
    <td>1</td>
    <td>0.7</td>
    <td>1</td>
    <td>1</td>
    <td>0.7</td>
    <td>1</td>
    <td>1</td>
    <td>1</td>
    <td>0.7</td>
  </tr>
  <tr>
    <td>num_round</td>
    <td>12</td>
    <td>12</td>
    <td>12</td>
    <td>12</td>
    <td>10</td>
    <td>12</td>
    <td>12</td>
    <td>12</td>
    <td>10</td>
    <td>10</td>
  </tr>
</table>
<br>
Again since the most accurate model is at the upper end of the swept ranges of max_depth=14 and num_round=12 we could try increasing these. However not all of the ten most accurate models have max_depth=14 or num_round=12 suggesting that increasing these values may not necessarily increase our accuracy and may in fact decrease it as in the top ten models it is not always true that lambda=0 and subsample=0.7, both parameters that attempt to reduce over fitting. So lets test the four most accurate models S2M1, S2M2, S2M3, and S2M4 in the next section.
</p>

<h1>Test Models</h1>
<hr>
<p>
Testing the models S2M1, S2M2, S2M3, and S2M4 on the testing set produces the following results, which suggests that the four models do not over fit the data as the models testing accuracy is at least as good as their validation accuracy.
<br><br>
<table class="center">
  <tr>
    <td>Model</td>
    <td>S2M1</td>
    <td>S2M2</td>
    <td>S2M3</td>
    <td>S2M4</td>
  </tr>
  <tr>
    <td>Accuracy</td>
    <td>0.7297</td>
    <td>0.7277</td>
    <td>0.7277</td>
    <td>0.7237</td>
  </tr>
</table>
<br>
The testing set I used here is not the same as the one used in the Kaggle competition so while its not a one to one comparison the winning model achieved an accuracy of 0.83216, see <a href="https://www.kaggle.com/c/whats-cooking/leaderboard">https://www.kaggle.com/c/whats-cooking/leaderboard</a>. This suggests either that I have chosen poor parameters for XGBoost or that the winning entries used different models.
</p>
<p>
The confusion matrix for model S2M1 is
<br><br>
<table>
	<tr>
		<td colspan="22" style="text-align:center"><b>Predicted Label</b></td>
	</tr>
	<tr>
		<td></td>
		<td></td>
		<td>gr</td>
		<td>so</td>
		<td>fi</td>
		<td>in</td>
		<td>ja</td>
		<td>sp</td>
		<td>me</td>
		<td>it</td>
		<td>br</td>
		<td>th</td>
		<td>vi</td>
		<td>ch</td>
		<td>ca</td>
		<td>bra</td>
		<td>fr</td>
		<td>ja</td>
		<td>ir</td>
		<td>ko</td>
		<td>mo</td>
		<td>ru</td>
	</tr>
	<tr>
		<td rowspan="20"><b>Label</b></td>
		<td>greek (gr)</td>
		<td>56</td>
		<td>4</td>
		<td>0</td>
		<td>3</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>31</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>southern_us (so)</td>
		<td>2</td>
		<td>322</td>
		<td>3</td>
		<td>3</td>
		<td>1</td>
		<td>2</td>
		<td>8</td>
		<td>36</td>
		<td>3</td>
		<td>1</td>
		<td>0</td>
		<td>3</td>
		<td>22</td>
		<td>1</td>
		<td>13</td>
		<td>1</td>
		<td>6</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>filipino (fi)</td>
		<td>0</td>
		<td>4</td>
		<td>37</td>
		<td>2</td>
		<td>1</td>
		<td>1</td>
		<td>2</td>
		<td>7</td>
		<td>1</td>
		<td>1</td>
		<td>2</td>
		<td>7</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>indian (in)</td>
		<td>4</td>
		<td>8</td>
		<td>0</td>
		<td>243</td>
		<td>1</td>
		<td>0</td>
		<td>16</td>
		<td>10</td>
		<td>3</td>
		<td>6</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>3</td>
		<td>0</td>
	</tr>
	<tr>
		<td>japanese (ja)</td>
		<td>0</td>
		<td>5</td>
		<td>3</td>
		<td>2</td>
		<td>28</td>
		<td>0</td>
		<td>2</td>
		<td>1</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>spanish (sp)</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>2</td>
		<td>1</td>
		<td>42</td>
		<td>15</td>
		<td>33</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>3</td>
		<td>0</td>
		<td>7</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
	</tr>
	<tr>
		<td>mexican (me)</td>
		<td>1</td>
		<td>14</td>
		<td>1</td>
		<td>6</td>
		<td>0</td>
		<td>3</td>
		<td>545</td>
		<td>30</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>1</td>
		<td>2</td>
		<td>5</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>1</td>
		<td>2</td>
	</tr>
	<tr>
		<td>italian (it)</td>
		<td>10</td>
		<td>31</td>
		<td>1</td>
		<td>2</td>
		<td>0</td>
		<td>3</td>
		<td>8</td>
		<td>715</td>
		<td>4</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>4</td>
		<td>0</td>
		<td>25</td>
		<td>1</td>
		<td>2</td>
		<td>1</td>
		<td>1</td>
		<td>0</td>
	</tr>
	<tr>
		<td>british (br)</td>
		<td>0</td>
		<td>26</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>11</td>
		<td>30</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>13</td>
		<td>0</td>
		<td>5</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
	</tr>
	<tr>
		<td>thai (th)</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>7</td>
		<td>0</td>
		<td>0</td>
		<td>7</td>
		<td>4</td>
		<td>0</td>
		<td>104</td>
		<td>10</td>
		<td>7</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>vietnamese (vi)</td>
		<td>1</td>
		<td>0</td>
		<td>2</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>3</td>
		<td>0</td>
		<td>20</td>
		<td>57</td>
		<td>6</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>chinese (ch)</td>
		<td>1</td>
		<td>4</td>
		<td>3</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>9</td>
		<td>2</td>
		<td>6</td>
		<td>5</td>
		<td>240</td>
		<td>1</td>
		<td>0</td>
		<td>1</td>
		<td>8</td>
		<td>0</td>
		<td>12</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>cajun_creole (ca)</td>
		<td>0</td>
		<td>28</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>6</td>
		<td>16</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>91</td>
		<td>0</td>
		<td>7</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>brazilian (bra)</td>
		<td>0</td>
		<td>8</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>1</td>
		<td>10</td>
		<td>5</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>33</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>french (fr)</td>
		<td>1</td>
		<td>40</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>4</td>
		<td>2</td>
		<td>62</td>
		<td>3</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>137</td>
		<td>0</td>
		<td>3</td>
		<td>0</td>
		<td>2</td>
		<td>3</td>
	</tr>
	<tr>
		<td>japanese (ja)</td>
		<td>0</td>
		<td>4</td>
		<td>1</td>
		<td>10</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>10</td>
		<td>2</td>
		<td>1</td>
		<td>0</td>
		<td>11</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>87</td>
		<td>1</td>
		<td>6</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>irish (ir)</td>
		<td>0</td>
		<td>21</td>
		<td>1</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>13</td>
		<td>7</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>6</td>
		<td>1</td>
		<td>20</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>korean (ko)</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>13</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>8</td>
		<td>0</td>
		<td>48</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>moroccan (mo)</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>7</td>
		<td>0</td>
		<td>3</td>
		<td>6</td>
		<td>6</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>51</td>
		<td>0</td>
	</tr>
	<tr>
		<td>russian (ru)</td>
		<td>3</td>
		<td>5</td>
		<td>1</td>
		<td>1</td>
		<td>0</td>
		<td>1</td>
		<td>4</td>
		<td>8</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>9</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>16</td>
	</tr>

</table>
<br>
Glancing at the confusion matrix it is apparent that this model often incorrectly predicts geek cuisine is italian, spanish cuisine is italian, british cuisine is southern_us, irish is southern_us, and russian cuisine is italian.
<br><br> 
<b>Plot ROC curves for each</b>
</p>
<h1>Feature Selection</h1>
<hr>
<p>
<b>For "best" parameter set using all features</b><br><br>
<b>Compare the following:</b><br><br>
<b>Full feature set</b><br><br>
<b>Feature set of the 50 most frequent ingredients for each cuisine (*)</b><br><br>
<b>delete nodes within 5% of max degree ingredient node in set defined in (*)</b><br><br>
<b>delete nodes within 10% of max degree ingredient node in set defined in (*)</b><br><br>
<b>delete nodes within 15% of max degree ingredient node in set defined in (*)</b><br><br>
<b>delete nodes within 20% of max degree ingredient node in set defined in (*)</b><br><br>
<b>delete nodes within 25% of max degree ingredient node in set defined in (*)</b><br><br>
<b>delete nodes within 30% of max degree ingredient node in set defined in (*)</b><br><br>
<b>delete nodes within 35% of max degree ingredient node in set defined in (*)</b><br><br>
<b>delete nodes within 40% of max degree ingredient node in set defined in (*)</b><br><br>
<b>delete nodes within 45% of max degree ingredient node in set defined in (*)</b><br><br>
<b>delete nodes within 50% of max degree ingredient node in set defined in (*)</b><br><br>
<b>delete nodes within 55% of max degree ingredient node in set defined in (*)</b><br><br>
<b>delete nodes within 60% of max degree ingredient node in set defined in (*)</b><br><br>
<b>delete nodes within 65% of max degree ingredient node in set defined in (*)</b><br><br>
<b>delete nodes within 70% of max degree ingredient node in set defined in (*)</b><br><br>
<b>Also notice what fraction of ingredients remain at the above points.<b>
<b>Try comparing highest degree nodes to nodes with ingredients with high mutual information??<b>
</p>
</div>
</body>
</html>
