<html>
<head>
<title>Data Exploration and XGBoost - Notes</title>
<link rel="stylesheet" type="text/css" href="styles.css">
<link rel="stylesheet" type="text/css" href="xgboost.css">
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div class="hspacer"></div>
<div class="content_block">
<h1>Introduction</h1>
<hr>
<p>
XGBoost is one of the ML models that I've frequently run across but never played around with. So this an attempt at getting familiar with the python implementation of XGBoost. Basically this involved following the docs here: <a href="https://xgboost.readthedocs.io/en/latest/"> https://xgboost.readthedocs.io/en/latest/</a>. The code I wrote for this project can be found here: <a href="https://github.com/sophist0/xgboost_recipes">https://github.com/sophist0/xgboost_recipes</a>.
</p>

<h1>Data Exploration</h1>
<hr>
<p>
I'm interested in cooking. So I decided to look for a dataset of recipes and found <a href="https://www.kaggle.com/kaggle/recipe-ingredients-dataset">https://www.kaggle.com/kaggle/recipe-ingredients-dataset</a>. 
</p>
<p>
This dataset consists of recipes classified by cuisine with the recipe ingredients as features. The portion of the dataset I'm using (the Kaggle training set) consists of 39774 recipes, 20 cuisines, and 6714 unique ingredients. Arguably this dataset could use cleaning in the sense that some features could be consolidated for instance "olive oil" and "extra-virgin olive oil", on the other hand different cuisines may use different words for identical or functionally identical ingredients. I chose not to consolidate any ingredients. 
</p>
<p>
The cuisines and their frequency in the dataset are given in the figure below.
<img class="img_cnr" src="images/cuisine_num_recipes_edit.png">
</p>
<p>
The twenty most frequent ingredients in all cuisine recipes in the dataset are given in the figure below.
<img class="img_cnr" src="images/ingredient_num_recipes_edit.png">
</p>
<p>
The eight most frequent ingredients of each cuisine which are not in the set of the eight most frequent ingredients of any other cuisine are given below.
<br>
<br>
<table class="center">
  <tr>
    <th><b>Cuisine</b></th>
    <th><b>Ingredients</b></th>
  </tr>
  <tr>
    <td>brazilian</td>
    <td>cachaca, lime</td>
  </tr>
  <tr>
    <td>british</td>
    <td>milk</td>
  </tr>
  <tr>
    <td>cajun_creole</td>
    <td>cajun seasoning, cayenne pepper, green bell pepper</td>
  </tr>
  <tr>
    <td>chinese</td>
    <td>corn starch</td>
  </tr>
  <tr>
    <td>filipino</td>
    <td>oil</td>
  </tr>
  <tr>
    <td>french</td>
    <td></td>
  </tr>
  <tr>
    <td>greek</td>
    <td>fresh lemon juice, feta cheese crumbles, dried oregano</td>
  </tr>
  <tr>
    <td>indian</td>
    <td>cumin seed, ground turmeric, garam masala</td>
  </tr>
  <tr>
    <td>irish</td>
    <td>baking soda, potatoes</td>
  </tr>
  <tr>
    <td>italian</td>
    <td>grated parmesan cheese</td>
  </tr>
  <tr>
    <td>jamaican</td>
    <td>dried thyme, scallions, ground allspice</td>
  </tr>
  <tr>
    <td>japanese</td>
    <td>rice vinegar, sake, mirin</td>
  </tr>
  <tr>
    <td>korean</td>
    <td>seseme seeds</td>
  </tr>
  <tr>
    <td>mexican</td>
    <td>jalapeno chilies, chili powder</td>
  </tr>
  <tr>
    <td>moroccan</td>
    <td>ground ginger, ground cinnamon</td>
  </tr>
  <tr>
    <td>russian</td>
    <td></td>
  </tr>
  <tr>
    <td>southern_us</td>
    <td></td>
  </tr>
  <tr>
    <td>spanish</td>
    <td>tomatoes</td>
  </tr>
  <tr>
    <td>thai</td>
    <td>coconut milk</td>
  </tr>
  <tr>
    <td>vietnamese</td>
    <td>shallots, carrots</td>
  </tr>
</table> 
<br>
If one fills in this table for the thirty most frequent ingredients rather than the eight most frequent, then every cuisine has ingredients in the ingredients column above. The eight most frequent elements produce nicer graphical representation, see below. But the point I'm trying to drive home is that if we only used the thirty most frequent ingredients of each cuisine we should be able to categorize a lot of recipes. In other words classifying recipes according to cuisine using their ingredients appears to be at least partially solvable problem. 
</p>
<p>
Below is a bipartite graph representation of subsets of the eight most frequent ingredients for each cuisine. The white nodes are the cuisines, the blue nodes are subsets ingredients, the edges indicate the subsets of each cuisines eight most ingredients. If there is more than one edge to a blue node, that nodes ingredients are in the set of the eight most frequent ingredients for each connected cuisine. For instance both Chinese and Korean recipes frequently use green onions and sesame oil.
<img class="img_cnr" src="images/cuisine_ingredients_n8.png">
<br>
Notice that the ingredients in the center of the graph such as garlic, onions, salt, sugar, and water are connected to most cuisines. Therefore their presence in a recipe likely tells us little about the recipes cuisine. Or more formally if all ingredient nodes represent sufficiently frequent ingredients, then highest degree nodes in the bipartite representation above likely have little power to classify recipes by cuisine. At least that's my currently baseless hypothesis.
</p>

<h1>XGBoost Paramater Selection</h1>
<hr>
<p>
XGBoost has a lot of parameters, see <a href="https://xgboost.readthedocs.io/en/latest/parameter.html">https://xgboost.readthedocs.io/en/latest/parameter.html</a>. I chose to focus on sweeping over the following parameters which produces 216 potential models.
<br><br>
<table class="center">
  <tr>
    <th><b>Parameter</b></th>
    <th><b>Description</b></th>
    <th><b>Range</b></th>
    <th><b>Default</b></th>
    <th><b>Sweep 1 Values</b></th>
  </tr>
  <tr>
    <td>eta</td>
    <td>learning rate</td>
    <td>[0,1]</td>
    <td>0.3</td>
    <td>0.1,0.5,1</td>
  </tr>
  <tr>
    <td>gamma</td>
    <td>min leaf loss to split a leaf</td>
    <td>[0,inf]</td>
    <td>0</td>
    <td>0,0.3</td>
  </tr>
  <tr>
    <td>max_depth</td>
    <td>maximum tree depth</td>
    <td>[0,inf]</td>
    <td>6</td>
    <td>2,4,6</td>
  </tr>
  <tr>
    <td>lambda</td>
    <td>L2 regulation</td>
    <td></td>
    <td>1</td>
    <td>0,1,3</td>
  </tr>
  <tr>
    <td>subsample</td>
    <td>fraction of data sampled before growing each tree</td>
    <td>(0,1]</td>
    <td>1</td>
    <td>0.7,1</td>
  </tr>
  <tr>
    <td>num_round</td>
    <td>number of boosting rounds usually 2</td>
    <td></td>
    <td></td>
    <td>2,4</td>
  </tr>
</table>
<br>
Setting aside 10% of the dataset as a testing set and using 5-fold cross validation, the models with top ten average accuracies and their parameter sets are given in the table below.
<br><br>
<table class="center">
  <tr>
    <td>Model</td>
    <td>S1M1</td>
    <td>S1M2</td>
    <td>S1M3</td>
    <td>S1M4</td>
    <td>S1M5</td>
    <td>S1M6</td>
    <td>S1M7</td>
    <td>S1M8</td>
    <td>S1M9</td>
    <td>S1M10</td>
  </tr>
  <tr>
    <td>Accuracy</td>
    <td>0.653</td>
    <td>0.653</td>
    <td>0.648</td>
    <td>0.648</td>
    <td>0.647</td>
    <td>0.646</td>
    <td>0.641</td>
    <td>0.641</td>
    <td>0.635</td>
    <td>0.634</td>
  </tr>
  <tr>
    <td>eta</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
  </tr>
  <tr>
    <td>gamma</td>
    <td>0</td>
    <td>0.3</td>
    <td>0</td>
    <td>0.3</td>
    <td>0</td>
    <td>0.3</td>
    <td>0</td>
    <td>0.3</td>
    <td>0</td>
    <td>0.3</td>
  </tr>
  <tr>
    <td>max_depth</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
    <td>6</td>
  </tr>
  <tr>
    <td>lambda</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>1</td>
    <td>1</td>
    <td>1</td>
    <td>3</td>
    <td>3</td>
  </tr>
  <tr>
    <td>subsample</td>
    <td>1</td>
    <td>1</td>
    <td>0.7</td>
    <td>0.7</td>
    <td>1</td>
    <td>1</td>
    <td>0.7</td>
    <td>0.7</td>
    <td>1</td>
    <td>1</td>
  </tr>
  <tr>
    <td>num_round</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
    <td>4</td>
  </tr>
</table>
</p>
<p>
Given this parameter sweep it appears we can fix eta=0.5 and it doesn't appear gamma has a large effect so lets fix it as gamma=0. Its also tempting to set lambda=0 and subsample=1 but I'm not going to fix those values as both parameters should reduce the chance of over fitting. Finally the fact that max_depth and num_round are at the maximum of their swept ranges suggest that they should both be increased and that the model is under fitting the validation data. So lets try another round of parameter fitting sweeping over the following values.
<br><br>
<table class="center">
  <tr>
    <th><b>Parameter</b></th>
    <th><b>Sweep 2 Values</b></th>
  </tr>
  <tr>
    <td>eta</td>
    <td>0.5</td>
  </tr>
  <tr>
    <td>gamma</td>
    <td>0</td>
  </tr>
  <tr>
    <td>max_depth</td>
    <td>8,10,12,14</td>
  </tr>
  <tr>
    <td>lambda</td>
    <td>0,1</td>
  </tr>
  <tr>
    <td>subsample</td>
    <td>0.7,1</td>
  </tr>
  <tr>
    <td>num_round</td>
    <td>6,8,10,12</td>
  </tr>
</table>
</p>
<p>
This gives 64 possible models. Again the top ten results are,
<br><br>
<table class="center">
  <tr>
    <td>Model</td>
    <td>S2M1</td>
    <td>S2M2</td>
    <td>S2M3</td>
    <td>S2M4</td>
    <td>S2M5</td>
    <td>S2M6</td>
    <td>S2M7</td>
    <td>S2M8</td>
    <td>S2M9</td>
    <td>S2M10</td>
  </tr>
  <tr>
    <td>Accuracy</td>
    <td>0.720</td>
    <td>0.718</td>
    <td>0.717</td>
    <td>0.717</td>
    <td>0.714</td>
    <td>0.713</td>
    <td>0.713</td>
    <td>0.713</td>
    <td>0.713</td>
    <td>0.713</td>
  </tr>
  <tr>
    <td>eta</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
    <td>0.5</td>
  </tr>
  <tr>
    <td>gamma</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>max_depth</td>
    <td>14</td>
    <td>12</td>
    <td>14</td>
    <td>14</td>
    <td>14</td>
    <td>12</td>
    <td>10</td>
    <td>12</td>
    <td>12</td>
    <td>14</td>
  </tr>
  <tr>
    <td>lambda</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>subsample</td>
    <td>1</td>
    <td>1</td>
    <td>0.7</td>
    <td>1</td>
    <td>1</td>
    <td>0.7</td>
    <td>1</td>
    <td>1</td>
    <td>1</td>
    <td>0.7</td>
  </tr>
  <tr>
    <td>num_round</td>
    <td>12</td>
    <td>12</td>
    <td>12</td>
    <td>12</td>
    <td>10</td>
    <td>12</td>
    <td>12</td>
    <td>12</td>
    <td>10</td>
    <td>10</td>
  </tr>
</table>
<br>
Again since the most accurate model is at the upper end of the swept ranges of max_depth=14 and num_round=12 we could try increasing these. However not all of the ten most accurate models have max_depth=14 or num_round=12 suggesting that increasing these values may not necessarily increase our accuracy and may in fact decrease it as in the top ten models it is not always true that lambda=0 and subsample=1. So in the next section lets test the four most accurate models S2M1, S2M2, S2M3, and S2M4 on the withheld test set.
</p>

<h1>Test Models</h1>
<hr>
<p>
Testing the models S2M1, S2M2, S2M3, and S2M4 on the testing set produces the following results, which suggests that the four models do not over fit the data as the models testing accuracies are at least as good as their validation accuracy.
<br><br>
<table class="center">
  <tr>
    <td>Model</td>
    <td>S2M1</td>
    <td>S2M2</td>
    <td>S2M3</td>
    <td>S2M4</td>
  </tr>
  <tr>
    <td>Accuracy</td>
    <td>0.7297</td>
    <td>0.7277</td>
    <td>0.7277</td>
    <td>0.7237</td>
  </tr>
</table>
<br>
The testing set I used here is not the same as the one used in the Kaggle competition so while its not a one to one comparison the winning model on Kaggle achieved an accuracy of 0.83216, see <a href="https://www.kaggle.com/c/whats-cooking/leaderboard">https://www.kaggle.com/c/whats-cooking/leaderboard</a>. This suggests either that I have chosen poor parameters for XGBoost or that the winning entries used different models.
</p>
<p>
The confusion matrix for model S2M1 is
<br><br>
<table class="no_b">
	<tr>
		<!-- <td class="no_b" colspan="22" style="text-align:center"><b>Predicted Label</b></td> --!>
		<td class="no_b" colspan="22"><div class="table_label"><b>Predicted Label</b></div></td>
	</tr>
	<tr>
		<td class="no_b"></td>
		<td class="no_blt"></td>
		<td>gr</td>
		<td>so</td>
		<td>fi</td>
		<td>in</td>
		<td>ja</td>
		<td>sp</td>
		<td>me</td>
		<td>it</td>
		<td>br</td>
		<td>th</td>
		<td>vi</td>
		<td>ch</td>
		<td>ca</td>
		<td>bra</td>
		<td>fr</td>
		<td>ja</td>
		<td>ir</td>
		<td>ko</td>
		<td>mo</td>
		<td>ru</td>
	</tr>
	<tr>
		<td class="no_blbt" rowspan="20"><b>Label</b></td>
		<td>greek (gr)</td>
		<td><b>56</b></td>
		<td>4</td>
		<td>0</td>
		<td>3</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>31</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>southern_us (so)</td>
		<td>2</td>
		<td><b>322</b></td>
		<td>3</td>
		<td>3</td>
		<td>1</td>
		<td>2</td>
		<td>8</td>
		<td>36</td>
		<td>3</td>
		<td>1</td>
		<td>0</td>
		<td>3</td>
		<td>22</td>
		<td>1</td>
		<td>13</td>
		<td>1</td>
		<td>6</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>filipino (fi)</td>
		<td>0</td>
		<td>4</td>
		<td><b>37</b></td>
		<td>2</td>
		<td>1</td>
		<td>1</td>
		<td>2</td>
		<td>7</td>
		<td>1</td>
		<td>1</td>
		<td>2</td>
		<td>7</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>indian (in)</td>
		<td>4</td>
		<td>8</td>
		<td>0</td>
		<td><b>243</b></td>
		<td>1</td>
		<td>0</td>
		<td>16</td>
		<td>10</td>
		<td>3</td>
		<td>6</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>3</td>
		<td>0</td>
	</tr>
	<tr>
		<td>japanese (ja)</td>
		<td>0</td>
		<td>5</td>
		<td>3</td>
		<td>2</td>
		<td><b>28</b></td>
		<td>0</td>
		<td>2</td>
		<td>1</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>spanish (sp)</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>2</td>
		<td>1</td>
		<td><b>42</b></td>
		<td>15</td>
		<td>33</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>3</td>
		<td>0</td>
		<td>7</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
	</tr>
	<tr>
		<td>mexican (me)</td>
		<td>1</td>
		<td>14</td>
		<td>1</td>
		<td>6</td>
		<td>0</td>
		<td>3</td>
		<td><b>545</b></td>
		<td>30</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>1</td>
		<td>2</td>
		<td>5</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>1</td>
		<td>2</td>
	</tr>
	<tr>
		<td>italian (it)</td>
		<td>10</td>
		<td>31</td>
		<td>1</td>
		<td>2</td>
		<td>0</td>
		<td>3</td>
		<td>8</td>
		<td><b>715</b></td>
		<td>4</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>4</td>
		<td>0</td>
		<td>25</td>
		<td>1</td>
		<td>2</td>
		<td>1</td>
		<td>1</td>
		<td>0</td>
	</tr>
	<tr>
		<td>british (br)</td>
		<td>0</td>
		<td>26</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>11</td>
		<td><b>30</b></td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>13</td>
		<td>0</td>
		<td>5</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
	</tr>
	<tr>
		<td>thai (th)</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>7</td>
		<td>0</td>
		<td>0</td>
		<td>7</td>
		<td>4</td>
		<td>0</td>
		<td><b>104</b></td>
		<td>10</td>
		<td>7</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>vietnamese (vi)</td>
		<td>1</td>
		<td>0</td>
		<td>2</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>3</td>
		<td>0</td>
		<td>20</td>
		<td><b>57</b></td>
		<td>6</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>chinese (ch)</td>
		<td>1</td>
		<td>4</td>
		<td>3</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>9</td>
		<td>2</td>
		<td>6</td>
		<td>5</td>
		<td><b>240</b></td>
		<td>1</td>
		<td>0</td>
		<td>1</td>
		<td>8</td>
		<td>0</td>
		<td>12</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>cajun_creole (ca)</td>
		<td>0</td>
		<td>28</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>6</td>
		<td>16</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td><b>91</b></td>
		<td>0</td>
		<td>7</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
	</tr>
	<tr>
		<td>brazilian (bra)</td>
		<td>0</td>
		<td>8</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>1</td>
		<td>10</td>
		<td>5</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td><b>33</b></td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>french (fr)</td>
		<td>1</td>
		<td>40</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>4</td>
		<td>2</td>
		<td>62</td>
		<td>3</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td><b>137</b></td>
		<td>0</td>
		<td>3</td>
		<td>0</td>
		<td>2</td>
		<td>3</td>
	</tr>
	<tr>
		<td>japanese (ja)</td>
		<td>0</td>
		<td>4</td>
		<td>1</td>
		<td>10</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>10</td>
		<td>2</td>
		<td>1</td>
		<td>0</td>
		<td>11</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td><b>87</b></td>
		<td>1</td>
		<td>6</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>irish (ir)</td>
		<td>0</td>
		<td><b>21</b></td>
		<td>1</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>13</td>
		<td>7</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>6</td>
		<td>1</td>
		<td>20</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>korean (ko)</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>13</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>8</td>
		<td>0</td>
		<td><b>48</b></td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>moroccan (mo)</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>7</td>
		<td>0</td>
		<td>3</td>
		<td>6</td>
		<td>6</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
		<td>1</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td><b>51</b></td>
		<td>0</td>
	</tr>
	<tr>
		<td>russian (ru)</td>
		<td>3</td>
		<td>5</td>
		<td>1</td>
		<td>1</td>
		<td>0</td>
		<td>1</td>
		<td>4</td>
		<td>8</td>
		<td>2</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>2</td>
		<td>0</td>
		<td>9</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td>0</td>
		<td><b>16</b></td>
	</tr>

</table>
<br>
Glancing at the confusion matrix it is apparent that this model often incorrectly classifies geek recipes as italian, spanish recipes as italian, british recipes as southern_us, irish recipes as southern_us, and russian recipes as italian.
<br><br> 
The ROC's for all four models are vary similar as demonstrated in the figure below. These ROC's where calculated by collapsing the multi-class problem into a binary problem. So for example in computing the ROC for the class 'italian' a true positive is any recipe correctly labeled as 'italian' and a false positive any recipe incorrectly labeled as 'italian'. I then averaged the ROC's for each class to produce the figure below. Arguably I should have weighted the ROC's according to the number of samples of each class in the dataset before averaging them together. This may explain why model S2M4 appears to dominate the others in much of the ROC space although it is a less accurate model.
<img class="img_cnr" src="images/rocs.png">
</p>
<h1>Feature Selection</h1>
<hr>
<p>
Looking at the bipartite graph above I thought perhaps ingredients in most cuisines could be ignored without reducing classification power. I decided to test this and found given the figure below that I was only partially correct.
</p>
<img class="img_cnr" src="images/fselect.png">
<p>
In this figure the blue line (#) represents the accuracy of model S2M1 predicting the cuisine of recipes using the most frequent ingredients in the dataset, the orange line ($) is the n most frequent ingredients in each cuisine, the green line (&) is the ingredients in the fewest cuisines that are in the set of the 100 most frequent ingredients of a cuisine, the red line (*) is the ingredients in the most cuisines that are in the set of the 100 most frequent ingredients of a cuisine.
</p>
<p>
Interestingly the orange (\$) line is strictly greater than the blue (#) line, suggesting its always better to select the most frequent n ingredients of each cuisine rather than the most frequent ingredients in the dataset. The green (&) and red (*) lines swept the fraction of ingredients selected from the orange (\$) line for n=100 from 0.05 to 1 in increments of 0.05. This implies that all three lines should converge at the right side of the graph, however there must be a bit of randomness in the model's training as this does not quite occur. But the real take away is that if the ingredients shared across the most cuisines are removed, initially it does not effect the models accuracy. So for instance perhaps removing water, which is an ingredient in all 20 cuisines, would not reduce the models accuracy. However this does not last long and as more ingredients are removed which are in many cuisine's, the models accuracy decreases substantially even falling below the red (*) line. Perhaps this is due to ingredients that are shared across a lot of cuisines being frequent ingredients, such that if 50% of them are removed a lot of recipes will have no ingredients making them impossible to classify. I have not verified this, its just a guess.
</p>
</div>
</body>
</html>
